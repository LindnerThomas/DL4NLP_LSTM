# -*- coding: utf-8 -*-
"""dl4nlp_ex01_question.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sX2PxKZ6kIIS4c0MTTvEhglz0vSa1wm2

# Deep Learning for NLP - Exercise 01
Building, Tuning and Evaluating a standard RNN Model

General hints:
* Have a look at the imports below when solving the tasks
* Use the given modules and all submodules of the imports, but don't import anything else!
    * For instance, you can use other functions under the `torch` or `nn` namespace, but don't import e.g. PyTorch Lightning, etc.
* It is recommended to install all packages from the provided environment file
* Feel free to test your code between sub-tasks of the exercise sheet, so that you can spot mistakes early (wrong shapes, impossible numbers, NaNs, ...)
* Just keep in mind that your final submission should be compliant to the provided initial format of this file

Submission guidelines:
* Make sure that the code runs on package versions from the the provided environment file
* Do not add or change any imports (also don't change the naming of imports, e.g. `torch.nn.functional as f`)
* Remove your personal, additional code testings and experiments throughout the notebook
* Do not change the class, function or naming structure as we will run tests on the given names
* Additionally export this notebook as a `.py` file, and submit **both** the executed `.ipynb` notebook with plots in it **and** the `.py` file
* **Deviation from the above guidelines will result in partial or full loss of points**

If you are using Google Colab or similar services, make sure to install all necessary packages so that the import cell below is working.

Usually, you would need to `!pip install`:
```
!pip install datasets==3.0.1
!pip install spacy==3.6.1
!pip install torch==2.0.1    # just to be sure we are all working with the same version
!pip install torchtext==0.15.2
!python -m spacy download en_core_web_sm
```

Make sure to comment out the lines before submitting!
"""

import os
import sys
import random
import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Sampler, BatchSampler, Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence

from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.utils import get_tokenizer

from datasets import load_dataset

def regularized_f1(train_f1, dev_f1, threshold=0.0015):
    """
    Returns development F1 if overfitting is below threshold, otherwise 0.
    """
    return dev_f1 if (train_f1 - dev_f1) < threshold else 0


def save_metrics(*args, path, fname):
    if not os.path.exists(path):
        os.makedirs(path)
    if not os.path.isfile(path + fname):
        with open(path + fname, "w", newline="\n") as f:
            f.write(
                ",".join(
                    [
                        "config",
                        "epoch",
                        "train_loss",
                        "train_acc",
                        "train_f1",
                        "val_loss",
                        "val_acc",
                        "val_f1",
                    ]
                )
            )
            f.write("\n")
    if args:
        with open(path + fname, "a", newline="\n") as f:
            f.write(",".join([str(arg) for arg in args]))
            f.write("\n")

def seed_everything(seed: int):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


seed_everything(1234)

VOCAB_SIZE = 20_000
BATCH_SIZE = 32
NUM_EPOCHS = 15
MAX_LEN = 256
LEARNING_RATE = 1e-4

"""## Task 1 - Download and prepare the dataset
- Load the train and test set of IMDB (it is included in the `datasets` module we imported above)
- Split the train set into train and validation set
    * Train set should consist of the middle 10% to 85% of data
    * Validation set should be the two remaining ends
    * You can achieve this slicing directly within the `load_dataset` function, check out the [Huggingface slicing API](https://huggingface.co/docs/datasets/v2.13.1/loading#slice-splits)
- Test set should stay unchanged
"""

# load dataset in splits
train_data = load_dataset("imdb", split="train[10%:85%]")
dev_data = load_dataset("imdb", split="train[:10%]+train[85%:]")
test_data = load_dataset("imdb", split="test")

"""* Define the tokenizer using `get_tokenizer` with spacy's `en_core_web_sm` module
    * You don't have to import spacy for that, but it is necessary to have spacy installed and the `en_core_web_sm` module downloaded
* Create the vocabulary using `build_vocab_from_iterator`
    * Think about which split(s) should be used to build the vocabulary
    * Include two special tokens: `'<UNK>'` at index `0`, `'<PAD>'` at index `1`
    * Limit the vocab size to `VOCAB_SIZE`, as defined in the beginning
    * Set the vocab's default returning index to `0` by making the `'<UNK>'` token default
    
Hint:
* This might be a good moment to add a personal test to check whether your vocab actually returns `0` for an unknown input token
"""

# define tokenizer
tokenizer = get_tokenizer("spacy", language="en_core_web_sm")

# define vocabulary
vocab = build_vocab_from_iterator(
    #use all the data for building the vocabulary to get the most exhaustive number of tokens?
    #Tokenize each
    (tokenizer(review) for review in train_data[:]["text"]),
    specials=['<UNK>', '<PAD>'],
    max_tokens= VOCAB_SIZE,
    special_first = True
)
vocab.set_default_index(vocab['<UNK>'])

vocab.get_itos()

"""* Use the tokenizer and vocabulary to turn your three data splits into indices
* Limit the maximum tokenized sequence length to `MAX_LEN`

**Note**:
In practice, performing this step on its own during the preprocessing stage is usually not feasible due to the memory constraints associated with storing large datasets. Consequently, the tokenization and indexing steps are typically performed "on the fly" within the `DataLoader`, specifically in the `collate_batch` step.
"""

#Each review is first tokenized to create a list of tokens and then mapped to the indices given by vocab
train_idx = [[vocab[token] for token in tokenizer(review["text"])[:MAX_LEN]] for review in train_data]
dev_idx = [[vocab[token] for token in tokenizer(review["text"])[:MAX_LEN]] for review in dev_data]
test_idx = [[vocab[token] for token in tokenizer(review["text"])[:MAX_LEN]] for review in test_data]

#TODO: remove
#Maximum number of tokens in review should be 256
max(len(inner_list) for inner_list in train_idx)

"""* Define a torch dataset by inhereting from `Dataset`
* It should create the building block to return the tokenized indices and labels for a given index
* Instantiate it
"""

class ImdbDataset(Dataset):
    def __init__(self, seq, lbl):
       self.data = seq
       self.labels = lbl

    def __getitem__(self, idx):
        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx])

    def __len__(self):
        return len(self.labels)

#stored the training data reviews (as a list containing sublists of tokens/integers) in the data field
# and the labels in the labels field
train_set = ImdbDataset(train_idx, train_data[:]["label"])

train_set.labels

"""* Having batches in which samples have a similar length, and thus less padding variations, improves training results
* A `GroupedSampler` allows us to create a sampler with which we can customize the data loading process
* It can then be implemented into the `DataLoader`, which automates loading data in multiple processes
* Write a sampler which allows us to group together samples of similar length into a batch
    * The `GroupedSampler` takes as input the tokenized sequences from `ImdbDataset`, as well as the batch size
    * First, in the `__init__` method, pair each sequence index with its tokenized sequence length
        * The result should be a list of tuples: `[(index, tokenized_sequence_length), ...]`
    * In the `__iter__` method, we now:
        * Shuffle the list
        * generate groups of size `BATCH_SIZE * 100`
        * Each group of size `BATCH_SIZE * 100` should be sorted in itself by the sequence length we calculated above
            * Sorting within each group is important because sorting based on the whole dataset would eliminate all training input variations
            * By shuffling in the `__iter__` method, we shuffle the set of indices in each new iteration (which equals an epoch), therefore, we keep input variation
        * The result should be a list of tuples sorted by ascending sequence length: `[(index, tokenized_sequence_length), ...]`
        * After each `BATCH_SIZE * 100` number of tuples, the sequence length of samples should drop and increase again
        * Example:
            ```
            Sample index 3199: (1234, 256)
            Sample index 3200: (567, 32)
            Sample index 3201: (890, 33)
            ```
        * Filter the created and sorted list to only consist of indices. Make sure to keep the sorting!
        * Return this list as an iterator
    * Complete the `__len__` method
"""

class GroupedSampler(Sampler):
    def __init__(self, seqs, batch_size):
        self.batch_size = batch_size
        self.seqs = seqs
        self.sequence_lengths = [(idx, len(sequence)) for (idx, sequence) in enumerate(seqs.data)]

    def __iter__(self):
        random.shuffle(self.sequence_lengths)
        #generate groups of size batchsize*100 as a list containing the sublists of size batchsize*100
        groups = [self.sequence_lengths[i:i + self.batch_size*100]
                    for i in range(0, len(self.sequence_lengths), self.batch_size*100)]
        #sort by the second tuple value in each of the sublists
        sorted_groups = [sorted(group, key=lambda x: x[1]) for group in groups]
        #until now I have a list of sublists but now it is supposed to be merged into one larger list
        #concatenated_groups = [item for sublist in sorted_groups for item in sublist]
        #only the index is returned for the final list, meaning only the first element of each tuple in the list
        final_groups = [tuple[0] for subgroup in sorted_groups for tuple in subgroup]
        #output should be a one-dimensional list
        return iter(final_groups)

    def __len__(self):
        return len(self.seqs)


"""* Now create the `GroupedSampler`, use it as input to create a `BatchSampler` (imported in the beginning)"""

train_grouped_sampler = GroupedSampler(seqs=train_set, batch_size=BATCH_SIZE)
train_sampler = BatchSampler(train_grouped_sampler, batch_size=BATCH_SIZE, drop_last=False)
#Each train_sampler instance returns a different set of batches for the training data (in total 586 batches of length 32)

len(list(train_sampler))

"""* Define a collate function which takes in a `batch` of tokenized sequences and labels created by the `BatchSampler`
    * Make sure to understand the structure of an input `batch`. Test around a bit to see what exactly they return.
* The collate function then:
    * pads these indices to the same length
        * use `padding_value=1`, `0` should be reserved for `UNK` token
    * turns the labels into tensors
    * finally, it creates a tensor which stores the length of all tokenized sentences **before** padding
    * the function should return 3 batched tensors: sequences, labels, lengths
"""

# define collate function
#QUESTION: It takes in a single batch or the whole set of batches as an epoch?
def collate_batch(batch):
    #list of labels and reviews (data) corresponding to the labels/reviews of the batch
    sequences, labels = zip(*batch)
    #batch_labels = torch.tensor([train_set.labels[idx] for idx in batch])// not necessary because the dataloader already does this for you
    #batch_data = torch.tensor([train_set.data[idx] for idx in batch])
    #list of lengths of each review in the batch
    batch_sequence_lengths = torch.tensor([len(seq) for seq in sequences])
    #batch_first = True or False?
    padded_data = pad_sequence(sequences=sequences, padding_value=1, batch_first=True)
    return (torch.tensor(padded_data), torch.tensor(labels),  batch_sequence_lengths)

"""* Now create the final `DataLoader` for the train set
    * For your training, set the number of workers to your liking/cpu cores setup
    * When submitting this exercise, please set `num_workers=2` at maximum
* Repeat the `DataLoader` creation process for the validation and test set
    * It is not necessary to introduce randomness into the validation and test set
    * Create an `ImdbDataset` and `DataLoader` instance
    * leave `shuffle` off and don't include any Samplers
    * still include the correct batch size and collate function
"""

# create dataloaders
train_loader = DataLoader(
    dataset=train_set,
    batch_sampler=train_sampler,
    collate_fn=collate_batch,
    num_workers=0,
)

dev_loader = DataLoader(
    dataset=ImdbDataset(dev_idx, dev_data[:]["label"]),
    batch_size=BATCH_SIZE,
    collate_fn=collate_batch,
    num_workers=0,
)

test_loader = DataLoader(
    dataset=ImdbDataset(test_idx, test_data[:]["label"]),
    batch_size=BATCH_SIZE,
    collate_fn=collate_batch,
    num_workers=0,
)

"""## Task 2 - Build Your Model
The model should consist of:
- an **embedding layer**, which takes `vocab_size` and `embedding_dim` as parameters
- a **dropout layer**, which takes `dropout` as a parameter
- an **LSTM layer**, which takes `embedding_dim` and `rnn_size` as parameters, and is bidirectional
- a **linear layer**, which takes the dimension of rnn output as input dimension and returns an output of `hidden_size` dimensions
- a **linear layer**, which first takes the previous layers output as input and returns one prediction per class of the dataset
- the output of the BiLSTM has hidden representation tensors for each index of each sequence. However, for the task of sequence classification, we just need one hidden representation tensor per sequence. Use `torch.mean()` as a pooling function for dimensionality reduction.
- use **dropout** on the embeddings and appropriate linear layer
- use **ReLU** as the activation function on the appropriate linear layer

_Hints:_
  - keep the position of the batch dimension equal across all layers
  - _use `pack_padded_sequence`_ and `pad_packed_sequence` at the appropriate steps. For more information, check out [this answer on stackoverflow](https://stackoverflow.com/a/56211056)
  - remember to include the `padding_idx=1` at relevant positions
  - as this is a binary classification task, it is possible to have 1 or 2 output neurons. Use your preference, but adjust the loss function towards your choice
"""

class BiLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, rnn_size, hidden_size, dropout):
        super().__init__()

        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embedding_dim,
            padding_idx=1
        )
        self.dropout = nn.Dropout(
            dropout,
        )
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=rnn_size,
            bidirectional=True,
            batch_first = True #because linear layer needs batch as 0-th dimension
        )
        self.linear1 = nn.Linear(
            in_features=2*rnn_size, #because lstm is bidirectional
            out_features=hidden_size
        )
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(
            in_features=hidden_size,
            out_features=1
        )



    def forward(self, seq, lengths):
        #Apply the embedding to each sequence -> embedding dimensionality
        seq = self.embedding(seq)
        seq = self.dropout(seq) # Use dropout on the embedding layer
        packed_sequences = pack_padded_sequence(seq, lengths=lengths, batch_first=True, enforce_sorted=False)
        output, (_, _) = self.lstm(packed_sequences)
        padded_output, _ = pad_packed_sequence(output, batch_first=True)
        #pad before or after the pooling?
        pooled_output = torch.mean(padded_output, dim=1)
        #output = all output hidden states of lstm ()
        seq = self.linear1(pooled_output)
        seq = self.dropout(seq)
        seq = self.relu(seq)
        seq = self.linear2(seq)
        return seq.squeeze()

"""## Task 3 - Inner train loop
* Create a global `device` variable which checks whether a GPU is available or not, and sets the device to either GPU or CPU.
"""

device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')

print(device)

"""* Write the inner train/test loop by completing the function `process`.
    * It takes the model, a dataloader, criterion and optionally the optimizer
    * The function iterates once through the dataloader, i.e. one epoch
    * Include the `tqdm` functionality for the loop through the dataloader by placing the loader inside `tqdm()`
        * Print its output to `file=sys.stdout`, and use `'batches'` as unit
        * You can also add a `desc='...'` to get a marking whether we currently train or evaluate
    * The function also moves the sequences and labels to `device`
    * The `lengths` need to stay on CPU!
    * If the optimizer is given, training with backpropagation is performed, then the below defined metrics are returned
    * If the optimizer is missing, evaluation is performed and the below described metrics are calculated
    * Values to be calculated:
        * Loss, Accuracy, both as averages of the total number of samples per epoch
        * F1 score between all predictions and labels of the epoch
"""

def process(model, loader, criterion, optim=None):
    total_loss = 0
    total_correct = 0
    total_samples = 0
    all_predictions = []
    all_labels = []

    if optim:
        model.train()
        desc = 'Training'
    else:
        model.eval()
        desc = 'Evaluating'

    for batch in tqdm(loader, file=sys.stdout, unit='batches', desc=desc):
        sequences, labels, lengths = batch
        sequences = sequences.to(device)
        labels = labels.to(device)
        lengths = lengths.cpu()

        batch_size = sequences.size(0)
        if batch_size != len(labels):
          raise ValueError(f"Dimension mismatch: sequences: {sequences.size()}, "
                               f"labels: {labels.size()}")

        if optim:
            optim.zero_grad()

        with torch.set_grad_enabled(optim is not None):
            outputs = model(sequences, lengths)
            loss = criterion(outputs.float(), labels.float())
            predictions = torch.round(torch.sigmoid(outputs.float()))

            if optim:
                loss.backward()
                optim.step()

        predicions = predictions.squeeze()
        total_loss += loss.item() * batch_size
        total_correct += (predictions.cpu() == labels.cpu()).sum().item() #sum over the number of correcct predictions per batch
        total_samples += batch_size #batch size is added to total samples each batch iteration

        all_predictions.extend(predictions.detach().cpu().numpy())
        all_labels.extend(labels.detach().cpu().numpy())

    avg_loss = total_loss / total_samples
    accuracy = total_correct / total_samples
    f1 = f1_score(all_labels, all_predictions)

    return avg_loss, accuracy, f1

"""# Task 4 - Training and Hyperparameter Optimization
In the following, we provide 3 configurations for the above created BiLSTM. Try to understand how they differ from each other.
"""

configs = {
    "config1": {
        "vocab_size": VOCAB_SIZE,
        "embedding_dim": 10,
        "hidden_size": 10,
        "rnn_size": 10,
        "dropout": 0.5
    },
    "config2": {
        "vocab_size": VOCAB_SIZE,
        "embedding_dim": 64,
        "hidden_size": 32,
        "rnn_size": 256,
        "dropout": 0.5
    },
    "config3": {
        "vocab_size": VOCAB_SIZE,
        "embedding_dim": 300,
        "hidden_size": 256,
        "rnn_size": 256,
        "dropout": 0.5
    }

}

for config_name, config in configs.items():
    print(f"Config name = {config_name}")
    print(config["embedding_dim"])

"""* Choose the correct criterion to train and evaluate your created model"""

criterion = nn.BCEWithLogitsLoss()

"""Use the given functions `regularized_f1` and `save_metrics` from the start of the notebook to implement the hyperparameter search and training runs.

Specifically:
* Iterate through each configuration
* Create and re-create the model for each new configuration run
    * Move the model to the `device`
* Create and re-create the optimizer with each new configuration model's paramaters
    * Use Adam as the optimizer
    * Use the learning rate defined at the beginning of notebook
* Train each configuration for `NUM_EPOCHS` epochs
* Change the model into train and evaluation mode at appropriate times
* Stop gradient calculation for evaluation runs
* Save metrics after each train and evaluation runs.
    * Have a look at the function to see what the expected inputs are
    * In the `.csv` file, only numbers should be entered
    * For instance, the inputs for the columns `config` and `epoch` should be e.g. `1`, _not_ `config1` or `epoch1`
* Optional: Print training progress for your own information

In order to check whether our model generalizes or just 'remembers', we need to compare the model's performance on the train set to the performance on the validation set. As we are only interested in non-overfitting performances, we only want to save model checkpoints when the model actually generalizes, i.e. has a higher F1 score on the validation set than on the train set.
* Calculate the regularized f1 score using the given function
* Keep track of multiple values during training:
    * Save the overall (i.e. across all configs *and* epochs) highest validation F1 score
        * Save your best model parameters
        * Overwrite your model parameters every time your model fulfills both the `regularized_f1` criteria and is better than the previous overall highest F1 score
        * In the end, the last saved `best_model.pt` parameters are automatically the best
        * Hint: Keep track (e.g. by printing or in a variable), which config produced the best model, so you can directly load that config for the test set run.
    * Track the highest F1 score inside a configuration but across epochs
        * Implement early-stopping for a configuration run if 3 consecutive epochs are below the highest F1 score for the current configuration
"""

path = './'
logging_file = 'results.csv'
best_val_f1 = 0
best_model_params = None
best_config = None
best_config_f1 = 0  # Initialize to 0 instead of None

print(f"Starting training with {len(configs)} configurations...")

for config_idx, (config_name, config) in enumerate(configs.items()):
    print(f"\nTraining configuration {config_idx + 1}/{len(configs)}")
    print(f"Config parameters: {config}")

    # Reset early stopping variables for each configuration
    epochs_below_best = 0
    best_config_f1 = 0

    # Create new model with the corresponding parameters
    model = BiLSTM(vocab_size=config["vocab_size"],
                   embedding_dim=config["embedding_dim"],
                   rnn_size=config["rnn_size"],
                   hidden_size=config["hidden_size"],
                   dropout=config["dropout"]
                   )
    model.to(device)

    # Create new optimizer
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Train for NUM_EPOCHS
    for epoch in range(NUM_EPOCHS):
        print(f"\nEpoch {epoch + 1}/{NUM_EPOCHS}")

        # Training phase
        train_loss, train_acc, train_f1 = process(model, train_loader, criterion, optimizer)
        print(f"Training - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")

        # Validation phase
        with torch.no_grad():
            val_loss, val_acc, val_f1 = process(model, dev_loader, criterion)
        print(f"Validation - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}")

        # Save metrics
        try:
            save_metrics(config_idx+1, epoch+1, train_loss, train_acc, train_f1,
                        val_loss, val_acc, val_f1, path=path, fname=logging_file)
        except Exception as e:
            print(f"Warning: Failed to save metrics: {e}")

        # Update best model if current one is better
        reg_f1 = regularized_f1(train_f1, val_f1)
        if reg_f1 > best_val_f1:
            best_val_f1 = reg_f1
            best_model_params = model.state_dict().copy()  # Make a copy of state dict
            best_config = config_idx + 1
            print(f"New best model found! Regularized F1: {reg_f1:.4f}")

        # Early stopping check
        if val_f1 > best_config_f1:
            best_config_f1 = val_f1
            epochs_below_best = 0
        else:
            epochs_below_best += 1

        # Early stopping
        if epochs_below_best >= 3:
            print(f"Early stopping triggered at epoch {epoch + 1} for configuration {config_idx + 1}")
            break

print("\nTraining completed!")
print(f"Best configuration: {best_config} with regularized F1: {best_val_f1:.4f}")

# Save the best model
if best_model_params is not None:
    try:
        model_save_path = os.path.join(path, "best_model.pt")
        torch.save({
            'model_state_dict': best_model_params,
            'config_idx': best_config,
            'best_val_f1': best_val_f1,
            'config': configs[list(configs.keys())[best_config-1]]  # Save the best config
        }, model_save_path)
        print(f"Best model saved to {model_save_path}")
    except Exception as e:
        print(f"Warning: Failed to save best model: {e}")

"""* Load in the created `results.csv` file
* Create 6 plots: 3 rows with each 2 sub-plots
* Each row of plots should correspond to a configuration
* Each left plot shows the loss progression per epoch
    * Include both train and evaluation progress in the same plot, but plot the evaluation lines dashed
    * Plot losses in blue
* Each right plot shows both the accuracy and F1 progression per epoch
    * Include both train and evaluation progess in the same plot, but plot the evaluation lines dashed
    * Plot accuracy in orange, and plot F1 in green
* Have a look at the example plot file included with this exercise. It constitutes one row of the plot.
* After plotting, briefly describe what problems and successes you see with each configuration
"""

df = pd.read_csv("results.csv")

fig, axes = plt.subplots(3, 2, figsize=(9, 11))

fig.suptitle('Training Metrics by Configuration', fontsize=16, y=0.95)

# Process each configuration
for config in range(1, 4):  # Configurations 1-3
    row = config - 1

    # Filter data for this configuration
    config_data = df[df['config'] == config]
    epochs = config_data['epoch']

    # Left plot: Losses
    # Training loss
    axes[row, 0].plot(epochs, config_data['train_loss'],
                      color='blue', label='Train Loss')
    # Validation loss
    axes[row, 0].plot(epochs, config_data['val_loss'],
                      color='blue', linestyle='--', label='Val Loss')

    axes[row, 0].set_title(f'Configuration {config} - Loss')
    axes[row, 0].set_xlabel('Epoch')
    axes[row, 0].set_ylabel('Loss')
    axes[row, 0].legend()
    axes[row, 0].grid(True)

    # Right plot: Accuracy and F1
    # Training metrics
    ln1 = axes[row, 1].plot(epochs, config_data['train_acc'],
                           color='orange', label='Train Acc')
    ln2 = axes[row, 1].plot(epochs, config_data['train_f1'],
                           color='green', label='Train F1')
    # Validation metrics
    ln3 = axes[row, 1].plot(epochs, config_data['val_acc'],
                           color='orange', linestyle='--', label='Val Acc')
    ln4 = axes[row, 1].plot(epochs, config_data['val_f1'],
                           color='green', linestyle='--', label='Val F1')

    axes[row, 1].set_title(f'Configuration {config} - Accuracy & F1')
    axes[row, 1].set_xlabel('Epoch')
    axes[row, 1].set_ylabel('Score')
    axes[row, 1].legend()
    axes[row, 1].grid(True)

plt.tight_layout()

# Analysis of each configuration
for config in range(1, 4):
    config_data = df[df['config'] == config]

    # Calculate key metrics
    final_train_loss = config_data['train_loss'].iloc[-1]
    final_val_loss = config_data['val_loss'].iloc[-1]
    best_val_f1 = config_data['val_f1'].max()
    best_val_acc = config_data['val_acc'].max()
    epochs_trained = len(config_data)

    print(f"\nConfiguration {config} Analysis:")
    print(f"Trained for {epochs_trained} epochs")
    print(f"Final training loss: {final_train_loss:.4f}")
    print(f"Final validation loss: {final_val_loss:.4f}")
    print(f"Best validation F1: {best_val_f1:.4f}")
    print(f"Best validation accuracy: {best_val_acc:.4f}")

    # Analyze problems and successes
    loss_gap = final_val_loss - final_train_loss
    early_stop = epochs_trained < max(df['epoch'])

    print("\nObservations:")
    if loss_gap > 0.5:  # High threshold for overfitting
        print("- Potential overfitting: large gap between training and validation loss")
    if early_stop:
        print("- Early stopping triggered: model stopped improving")
    if config_data['val_f1'].iloc[-3:].std() < 0.01:  # Check if F1 plateaued
        print("- Model converged: F1 score stabilized")
    if best_val_f1 > 0.8:
        print("- Strong performance: achieved high F1 score")
    elif best_val_f1 < 0.6:
        print("- Poor performance: low F1 score suggests model struggled to learn")

    # Check learning dynamics
    if config_data['train_loss'].iloc[0] > config_data['train_loss'].iloc[-1]:
        print("- Successful learning: loss decreased during training")
    else:
        print("- Learning issues: loss did not decrease significantly")

plt.show()

"""___

Student answer here:
___

* As the final step, instantiate a model with the config of the best run
* Load the `state_dict`
* Evaluate it on the test set
    * Don't forget to actviate evaluation mode and deactivate gradient calculation
* Comment on the performance
    * Did it generalize well? Why? Why not?
    * What could be done to improve the performance even further?
        * Consider also the hyperparameters from the third cell and discuss potential tradeoffs.
"""

# evaluate on test set
#model = #TODO

"""___

Student answer here:
___
"""

